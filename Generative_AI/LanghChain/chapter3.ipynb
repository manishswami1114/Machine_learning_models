{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b7c86f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_postgres.vectorstores import PGVector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a6c217",
   "metadata": {},
   "source": [
    "### Generating LLM Predictions Using Relevant Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fa4f9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding model\n",
    "embedding_llm = OllamaEmbeddings(model=\"mxbai-embed-large:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aa11eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data and\n",
    "raw_documents = TextLoader(\"./text.txt\", encoding=\"utf-8\").load()\n",
    "# split the data into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "# connect the postgres instance with PGvector\n",
    "\n",
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "db = PGVector.from_documents(\n",
    "    documents, embedding_llm, connection=connection, pre_delete_collection=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b5c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Retriever to retrieve text\n",
    "retriever = db.as_retriever()\n",
    "docs = retriever.invoke(\"who are the key figure in the fungi and natural agriculture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec365f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b788d64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Retriever to retrieve text\n",
    "# using search_kwargs=2 get 2 document\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 2})\n",
    "query = \"who are the key figure in the fungi and natural agriculture\"\n",
    "docs = retriever.invoke(\"who are the key figure in the fungi and natural agriculture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47039d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db4036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import context\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = OllamaLLM(model=\"qwen3:1.7b\")\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question based on following context: {context} Question: {question}\"\"\"\n",
    ")\n",
    "llm_chain = prompt | llm\n",
    "# answer the question based relevent document\n",
    "result = llm_chain.invoke({\"context\": docs, \"question\": query})\n",
    "\n",
    "print(result)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00229a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encapsulate this retrival logic in a single funtion\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "\n",
    "@chain\n",
    "def qa(input):\n",
    "    docs = retriever.invoke(input)\n",
    "\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "\n",
    "result = qa.invoke(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58d1ad3",
   "metadata": {},
   "source": [
    "### Query Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caceabe0",
   "metadata": {},
   "source": [
    "#### Rewrite-Retrieve-Read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5ff8c8",
   "metadata": {},
   "source": [
    "***Rewrite the user's query before performing retrival***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc54fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chain\n",
    "def qa(input):\n",
    "    docs = retriever.get_relevant_documents(input)\n",
    "\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "\n",
    "qa.invoke(\n",
    "    \"Today I woke up and brushed my teeth, then I sat down to read the news. But then I forgot the food on the cooker. Who are some key figures in the ancient agriculture and philosophy?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d481056",
   "metadata": {},
   "source": [
    "**In upperside code the LLM failed to answer the question because it was distracted by the irrelevant information provided in the query**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ddc773",
   "metadata": {},
   "source": [
    "***Now implement the Rewrite-retrieve-read***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce98ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\" Provide a better search query for web search engine to answer the given question, end the queries with ’**’. Question: {x} Answer:\n",
    " \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def parse_rewrite_output(message):\n",
    "    return message.strip('\"').strip(\"**\")\n",
    "\n",
    "\n",
    "rewriter = rewrite_prompt | llm | parse_rewrite_output\n",
    "\n",
    "\n",
    "@chain\n",
    "def qa_rrr(input):\n",
    "    # rewrite the query\n",
    "\n",
    "    new_query = rewriter.invoke(input)\n",
    "\n",
    "    # fetch relevant documents\n",
    "    docs = retriever.aget_relevant_documents(new_query)\n",
    "\n",
    "    # format prompt\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "\n",
    "    answer = llm.invoke(formatted)\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "qa_rrr.invoke(\n",
    "    \"\"\"\n",
    "    Today I woke up and brushed my teeth, then I sat down to read the news. But then I forgot the food on the cooker. Who are some key figures in the ancient agriculture history of philosophy?\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8686aa",
   "metadata": {},
   "source": [
    "#### Multi-Query Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2e6eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "perspectives_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an AI language model assistant. Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database. \n",
    "    By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based  similarity search. \n",
    "    Provide these alternative questions separated by newlines. \n",
    "    Original question: {question}\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def parse_queries_output(message):\n",
    "    return message.split(\"\\n\")\n",
    "\n",
    "\n",
    "query_gen = perspectives_prompt | llm | parse_queries_output\n",
    "\n",
    "\n",
    "def get_unique_union(document_lists):\n",
    "    deduped_docs = {\n",
    "        doc.page_content: doc for sublist in document_lists for doc in sublist\n",
    "    }\n",
    "    return list(deduped_docs.values())\n",
    "\n",
    "\n",
    "retrieval_chain = query_gen | retriever.batch | get_unique_union\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the question based only on the following context: {context} Question: {question} \n",
    "    \"\"\"\n",
    ")\n",
    "query = \"Who are the key figures in the ancient greek history of philosophy?\"\n",
    "\n",
    "\n",
    "@chain\n",
    "def multi_query_qa(input):\n",
    "    docs = retrieval_chain.invoke(input)\n",
    "\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "\n",
    "print(\"Running multi query qa\\n\")\n",
    "result = multi_query_qa.invoke(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9215694f",
   "metadata": {},
   "source": [
    "#### RAG-Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a4347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n Generate multiple search queries related to: {question} \\n Output (4 queries):\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def parse_queries_output(message):\n",
    "    return message.split(\"\\n\")\n",
    "\n",
    "\n",
    "query_gen = prompt_rag_fusion | llm | parse_queries_output\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(result: list[list], k=60):\n",
    "    fused_scores = {}\n",
    "    documents = {}\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = doc.page_content\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "                documents[doc_str] = doc\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "    reranked_doc_strs = sorted(\n",
    "        fused_scores, key=lambda d: fused_scores[d], reverse=True\n",
    "    )\n",
    "    return [documents[doc_str] for doc_str in reranked_doc_strs]\n",
    "\n",
    "\n",
    "retrivsl_chain = query_gen | retriever.batch | reciprocal_rank_fusion\n",
    "\n",
    "result = retrieval_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479ac7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"retrieved context using rank fusion:\\n\", result[0].page_content)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"Use model to answer question based on retrieved docs\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1d0fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the question based only on the following context: {context} Question: {question} \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "@chain\n",
    "def rag_fusion(input):\n",
    "    docs = retrieval_chain.invoke(input)\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "\n",
    "# run\n",
    "print(\"Running rag fusion\\n\")\n",
    "result = rag_fusion.invoke(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41458848",
   "metadata": {},
   "source": [
    "#### Hypothetical Document Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9020d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt_hyde = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Please write a passage to answer the question.\\n Question: {question} \\n Passage:\n",
    "    \"\"\"\n",
    ")\n",
    "generate_doc = prompt_hyde | llm | StrOutputParser()\n",
    "\n",
    "retrieval_chain = generate_doc | retriever\n",
    "\n",
    "query = (\n",
    "    \"Who are some lesser known philosophers in the ancient greek history of philosophy?\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question based only on the following context: {context} Question: {question} \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "@chain\n",
    "def qa(input):\n",
    "    docs = retrieval_chain.invoke(input)\n",
    "\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "\n",
    "print(\"Running hyde\\n\")\n",
    "result = qa.invoke(query)\n",
    "print(\"\\n\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9b5438",
   "metadata": {},
   "source": [
    "### Query Routing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174889b9",
   "metadata": {},
   "source": [
    "#### Logical Routing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dde28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bebeee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# LLM\n",
    "llm = OllamaLLM(model=\"llama3.2:3b\")\n",
    "\n",
    "\n",
    "# Schema\n",
    "class RouteQuery(BaseModel):\n",
    "    datasource: Literal[\"python_docs\", \"js_docs\"] = Field(\n",
    "        ...,\n",
    "        description=\"Choose which datasource would be most relevant for answering the user question.\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Parser\n",
    "parser = PydanticOutputParser(pydantic_object=RouteQuery)\n",
    "\n",
    "# Prompt with format instructions injected once\n",
    "system = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
    "Return ONLY valid JSON matching the schema. No explanations, no markdown, no extra text.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system), (\"human\", \"{question}\\n\\n{format_instructions}\")]\n",
    ").partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "# Router\n",
    "router = prompt | llm | parser\n",
    "\n",
    "# Example question\n",
    "question = \"\"\"why doesn't the following code work:\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"]) \n",
    "prompt.invoke(\"french\")\"\"\"\n",
    "\n",
    "# Run router\n",
    "result = router.invoke({\"question\": question})\n",
    "print(\"\\nRouting to:\", result)\n",
    "\n",
    "\n",
    "# Route chooser\n",
    "def choose_route(result: RouteQuery):\n",
    "    if \"python_docs\" in result.datasource.lower():\n",
    "        return \"chain for python_docs\"\n",
    "    else:\n",
    "        return \"chain for js_docs\"\n",
    "\n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route)\n",
    "\n",
    "final_result = full_chain.invoke({\"question\": question})\n",
    "print(\"\\nChoose route:\", final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c79a468",
   "metadata": {},
   "source": [
    "#### Sementic Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94bb14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "\n",
    "physics_template = \"\"\"You are a very smart physics professor. You are great at     answering questions about physics in a concise and easy-to-understand manner.     When you don't know the answer to a question, you admit that you don't know. Here is a question: {query}\"\"\"\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering     math questions. You are so good because you are able to break down hard     problems into their component parts, answer the component parts, and then     put them together to answer the broader question. Here is a question: {query}\"\"\"\n",
    "\n",
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embeddings = embedding_llm.embed_documents(prompt_templates)\n",
    "\n",
    "\n",
    "@chain\n",
    "def prompt_router(query):\n",
    "    query_embedding = embedding_llm.embed_query(query)\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    print(\"Using Math\" if most_similar == math_template else \"Using Physics\")\n",
    "    return PromptTemplate.from_template(most_similar)\n",
    "\n",
    "\n",
    "semantic_router = prompt_router | llm | StrOutputParser()\n",
    "\n",
    "result = semantic_router.invoke(\"what's a black hole\")\n",
    "print(\"\\nSementic router result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a1c071",
   "metadata": {},
   "source": [
    "### Query Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d8a2ed",
   "metadata": {},
   "source": [
    "#### Text-to-Metadata filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52ba3dc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Cannot import lark, please install it with 'pip install lark'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     48\u001b[39m fields = [\n\u001b[32m     49\u001b[39m     AttributeInfo(\n\u001b[32m     50\u001b[39m         name=\u001b[33m\"\u001b[39m\u001b[33mgenre\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     68\u001b[39m     ),\n\u001b[32m     69\u001b[39m ]\n\u001b[32m     71\u001b[39m description = \u001b[33m\"\u001b[39m\u001b[33mBrief summary of a movie\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Fixed typo\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m retriever = \u001b[43mSelfQueryRetriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28mprint\u001b[39m(retriever.invoke(\u001b[33m\"\u001b[39m\u001b[33mI want to watch a movie rated higher than 8.5\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     75\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/GEN_ENV/lib/python3.11/site-packages/langchain/retrievers/self_query/base.py:381\u001b[39m, in \u001b[36mSelfQueryRetriever.from_llm\u001b[39m\u001b[34m(cls, llm, vectorstore, document_contents, metadata_field_info, structured_query_translator, chain_kwargs, enable_limit, use_original_query, **kwargs)\u001b[39m\n\u001b[32m    374\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    375\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallowed_operators\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m chain_kwargs\n\u001b[32m    376\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m structured_query_translator.allowed_operators \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m ):\n\u001b[32m    378\u001b[39m     chain_kwargs[\u001b[33m\"\u001b[39m\u001b[33mallowed_operators\u001b[39m\u001b[33m\"\u001b[39m] = (\n\u001b[32m    379\u001b[39m         structured_query_translator.allowed_operators\n\u001b[32m    380\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m query_constructor = \u001b[43mload_query_constructor_runnable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocument_contents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata_field_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mchain_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    388\u001b[39m query_constructor = query_constructor.with_config(\n\u001b[32m    389\u001b[39m     run_name=QUERY_CONSTRUCTOR_RUN_NAME,\n\u001b[32m    390\u001b[39m )\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m    392\u001b[39m     query_constructor=query_constructor,\n\u001b[32m    393\u001b[39m     vectorstore=vectorstore,\n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m     **kwargs,\n\u001b[32m    397\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/GEN_ENV/lib/python3.11/site-packages/langchain/chains/query_constructor/base.py:370\u001b[39m, in \u001b[36mload_query_constructor_runnable\u001b[39m\u001b[34m(llm, document_contents, attribute_info, examples, allowed_comparators, allowed_operators, enable_limit, schema_prompt, fix_invalid, **kwargs)\u001b[39m\n\u001b[32m    356\u001b[39m prompt = get_query_constructor_prompt(\n\u001b[32m    357\u001b[39m     document_contents,\n\u001b[32m    358\u001b[39m     attribute_info,\n\u001b[32m   (...)\u001b[39m\u001b[32m    364\u001b[39m     **kwargs,\n\u001b[32m    365\u001b[39m )\n\u001b[32m    366\u001b[39m allowed_attributes = [\n\u001b[32m    367\u001b[39m     ainfo.name \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ainfo, AttributeInfo) \u001b[38;5;28;01melse\u001b[39;00m ainfo[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    368\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ainfo \u001b[38;5;129;01min\u001b[39;00m attribute_info\n\u001b[32m    369\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m output_parser = \u001b[43mStructuredQueryOutputParser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_components\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallowed_comparators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallowed_comparators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallowed_operators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallowed_operators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallowed_attributes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallowed_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfix_invalid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfix_invalid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prompt | llm | output_parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/GEN_ENV/lib/python3.11/site-packages/langchain/chains/query_constructor/base.py:103\u001b[39m, in \u001b[36mStructuredQueryOutputParser.from_components\u001b[39m\u001b[34m(cls, allowed_comparators, allowed_operators, allowed_attributes, fix_invalid)\u001b[39m\n\u001b[32m     95\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m fix_filter_directive(\n\u001b[32m     96\u001b[39m             filter_directive,\n\u001b[32m     97\u001b[39m             allowed_comparators=allowed_comparators,\n\u001b[32m     98\u001b[39m             allowed_operators=allowed_operators,\n\u001b[32m     99\u001b[39m             allowed_attributes=allowed_attributes,\n\u001b[32m    100\u001b[39m         )\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     ast_parse = \u001b[43mget_parser\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallowed_comparators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallowed_comparators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallowed_operators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallowed_operators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallowed_attributes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallowed_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.parse\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(ast_parse=ast_parse)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/GEN_ENV/lib/python3.11/site-packages/langchain/chains/query_constructor/parser.py:204\u001b[39m, in \u001b[36mget_parser\u001b[39m\u001b[34m(allowed_comparators, allowed_operators, allowed_attributes)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m QueryTransformer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    203\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mCannot import lark, please install it with \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpip install lark\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m    205\u001b[39m transformer = QueryTransformer(\n\u001b[32m    206\u001b[39m     allowed_comparators=allowed_comparators,\n\u001b[32m    207\u001b[39m     allowed_operators=allowed_operators,\n\u001b[32m    208\u001b[39m     allowed_attributes=allowed_attributes,\n\u001b[32m    209\u001b[39m )\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Lark(GRAMMAR, parser=\u001b[33m\"\u001b[39m\u001b[33mlalr\u001b[39m\u001b[33m\"\u001b[39m, transformer=transformer, start=\u001b[33m\"\u001b[39m\u001b[33mprogram\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: Cannot import lark, please install it with 'pip install lark'."
     ]
    }
   ],
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_ollama import OllamaLLM, OllamaEmbeddings\n",
    "\n",
    "# Define the missing variables\n",
    "llm = OllamaLLM(model=\"llama3.2:3b\")  # or whatever model you want to use\n",
    "embedding_llm = OllamaEmbeddings(model=\"mxbai-embed-large:latest\")\n",
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
    "        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n",
    "        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n",
    "        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n",
    "        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Toys come alive and have a blast doing so\",\n",
    "        metadata={\"year\": 1995, \"genre\": \"animated\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n",
    "        metadata={\n",
    "            \"year\": 1979,\n",
    "            \"director\": \"Andrei Tarkovsky\",\n",
    "            \"genre\": \"thriller\",\n",
    "            \"rating\": 9.9,\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create vectorstore\n",
    "vectorstore = PGVector.from_documents(docs, embedding_llm, connection=connection)\n",
    "\n",
    "fields = [\n",
    "    AttributeInfo(\n",
    "        name=\"genre\",\n",
    "        description=\"The genre of the movie\",\n",
    "        type=\"string or list[string]\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The year the movie was released\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"director\",\n",
    "        description=\"The name of the movie director\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\",\n",
    "        description=\"A 1-10 rating for the movie\",\n",
    "        type=\"float\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "description = \"Brief summary of a movie\"  # Fixed typo\n",
    "retriever = SelfQueryRetriever.from_llm(llm, vectorstore, description, fields)\n",
    "\n",
    "print(retriever.invoke(\"I want to watch a movie rated higher than 8.5\"))\n",
    "print(\"\\n\")\n",
    "print(retriever.invoke(\"what's a highly rated (above 8.5) science fiction film?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d783cee4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Cannot import lark, please install it with 'pip install lark'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     48\u001b[39m fields = [\n\u001b[32m     49\u001b[39m     AttributeInfo(\n\u001b[32m     50\u001b[39m         name=\u001b[33m\"\u001b[39m\u001b[33mgenre\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     68\u001b[39m     ),\n\u001b[32m     69\u001b[39m ]\n\u001b[32m     71\u001b[39m description = \u001b[33m\"\u001b[39m\u001b[33mBrief summary of a movie\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Fixed typo\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m retriever = \u001b[43mSelfQueryRetriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28mprint\u001b[39m(retriever.invoke(\u001b[33m\"\u001b[39m\u001b[33mI want to watch a movie rated higher than 8.5\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     75\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/GEN_ENV/lib/python3.11/site-packages/langchain/retrievers/self_query/base.py:381\u001b[39m, in \u001b[36mSelfQueryRetriever.from_llm\u001b[39m\u001b[34m(cls, llm, vectorstore, document_contents, metadata_field_info, structured_query_translator, chain_kwargs, enable_limit, use_original_query, **kwargs)\u001b[39m\n\u001b[32m    374\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    375\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallowed_operators\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m chain_kwargs\n\u001b[32m    376\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m structured_query_translator.allowed_operators \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m ):\n\u001b[32m    378\u001b[39m     chain_kwargs[\u001b[33m\"\u001b[39m\u001b[33mallowed_operators\u001b[39m\u001b[33m\"\u001b[39m] = (\n\u001b[32m    379\u001b[39m         structured_query_translator.allowed_operators\n\u001b[32m    380\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m query_constructor = \u001b[43mload_query_constructor_runnable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocument_contents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata_field_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mchain_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    388\u001b[39m query_constructor = query_constructor.with_config(\n\u001b[32m    389\u001b[39m     run_name=QUERY_CONSTRUCTOR_RUN_NAME,\n\u001b[32m    390\u001b[39m )\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m    392\u001b[39m     query_constructor=query_constructor,\n\u001b[32m    393\u001b[39m     vectorstore=vectorstore,\n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m     **kwargs,\n\u001b[32m    397\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/GEN_ENV/lib/python3.11/site-packages/langchain/chains/query_constructor/base.py:370\u001b[39m, in \u001b[36mload_query_constructor_runnable\u001b[39m\u001b[34m(llm, document_contents, attribute_info, examples, allowed_comparators, allowed_operators, enable_limit, schema_prompt, fix_invalid, **kwargs)\u001b[39m\n\u001b[32m    356\u001b[39m prompt = get_query_constructor_prompt(\n\u001b[32m    357\u001b[39m     document_contents,\n\u001b[32m    358\u001b[39m     attribute_info,\n\u001b[32m   (...)\u001b[39m\u001b[32m    364\u001b[39m     **kwargs,\n\u001b[32m    365\u001b[39m )\n\u001b[32m    366\u001b[39m allowed_attributes = [\n\u001b[32m    367\u001b[39m     ainfo.name \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ainfo, AttributeInfo) \u001b[38;5;28;01melse\u001b[39;00m ainfo[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    368\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ainfo \u001b[38;5;129;01min\u001b[39;00m attribute_info\n\u001b[32m    369\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m output_parser = \u001b[43mStructuredQueryOutputParser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_components\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallowed_comparators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallowed_comparators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallowed_operators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallowed_operators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallowed_attributes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallowed_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfix_invalid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfix_invalid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prompt | llm | output_parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/GEN_ENV/lib/python3.11/site-packages/langchain/chains/query_constructor/base.py:103\u001b[39m, in \u001b[36mStructuredQueryOutputParser.from_components\u001b[39m\u001b[34m(cls, allowed_comparators, allowed_operators, allowed_attributes, fix_invalid)\u001b[39m\n\u001b[32m     95\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m fix_filter_directive(\n\u001b[32m     96\u001b[39m             filter_directive,\n\u001b[32m     97\u001b[39m             allowed_comparators=allowed_comparators,\n\u001b[32m     98\u001b[39m             allowed_operators=allowed_operators,\n\u001b[32m     99\u001b[39m             allowed_attributes=allowed_attributes,\n\u001b[32m    100\u001b[39m         )\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     ast_parse = \u001b[43mget_parser\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallowed_comparators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallowed_comparators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallowed_operators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallowed_operators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallowed_attributes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallowed_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.parse\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(ast_parse=ast_parse)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/GEN_ENV/lib/python3.11/site-packages/langchain/chains/query_constructor/parser.py:204\u001b[39m, in \u001b[36mget_parser\u001b[39m\u001b[34m(allowed_comparators, allowed_operators, allowed_attributes)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m QueryTransformer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    203\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mCannot import lark, please install it with \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpip install lark\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m    205\u001b[39m transformer = QueryTransformer(\n\u001b[32m    206\u001b[39m     allowed_comparators=allowed_comparators,\n\u001b[32m    207\u001b[39m     allowed_operators=allowed_operators,\n\u001b[32m    208\u001b[39m     allowed_attributes=allowed_attributes,\n\u001b[32m    209\u001b[39m )\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Lark(GRAMMAR, parser=\u001b[33m\"\u001b[39m\u001b[33mlalr\u001b[39m\u001b[33m\"\u001b[39m, transformer=transformer, start=\u001b[33m\"\u001b[39m\u001b[33mprogram\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: Cannot import lark, please install it with 'pip install lark'."
     ]
    }
   ],
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_ollama import OllamaLLM, OllamaEmbeddings\n",
    "\n",
    "# Define the missing variables\n",
    "llm = OllamaLLM(model=\"llama3.2:3b\")  # or whatever model you want to use\n",
    "embedding_llm = OllamaEmbeddings(model=\"mxbai-embed-large:latest\")\n",
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
    "        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n",
    "        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n",
    "        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n",
    "        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Toys come alive and have a blast doing so\",\n",
    "        metadata={\"year\": 1995, \"genre\": \"animated\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n",
    "        metadata={\n",
    "            \"year\": 1979,\n",
    "            \"director\": \"Andrei Tarkovsky\",\n",
    "            \"genre\": \"thriller\",\n",
    "            \"rating\": 9.9,\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create vectorstore\n",
    "vectorstore = PGVector.from_documents(docs, embedding_llm, connection=connection)\n",
    "\n",
    "fields = [\n",
    "    AttributeInfo(\n",
    "        name=\"genre\",\n",
    "        description=\"The genre of the movie\",\n",
    "        type=\"string or list[string]\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The year the movie was released\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"director\",\n",
    "        description=\"The name of the movie director\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\",\n",
    "        description=\"A 1-10 rating for the movie\",\n",
    "        type=\"float\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "description = \"Brief summary of a movie\"  # Fixed typo\n",
    "retriever = SelfQueryRetriever.from_llm(llm, vectorstore, description, fields)\n",
    "\n",
    "print(retriever.invoke(\"I want to watch a movie rated higher than 8.5\"))\n",
    "print(\"\\n\")\n",
    "print(retriever.invoke(\"what's a highly rated (above 8.5) science fiction film?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67e069a",
   "metadata": {},
   "source": [
    "#### Text-to-Sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf077371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import QuerySQLDataBaseTool\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain.chains import create_sql_query_chain\n",
    "\n",
    "db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n",
    "\n",
    "print(db.get_usable_table_names())\n",
    "\n",
    "write_query = create_sql_query_chain(llm, db)\n",
    "\n",
    "execute_query = QuerySQLDataBaseTool(db=db)\n",
    "\n",
    "combined_chain = write_query | execute_query\n",
    "\n",
    "result = combined_chain.invoke({\"question\": \"How many employees are there\"})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0114e0d5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GEN_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
